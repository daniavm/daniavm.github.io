---
title: "En Busca del Modelo Perdido - Parte 5: Afinando el Random Forest"
layout: single
author_profile: true
related: true
comments: true
toc: true
toc_icon: "egg"

header:
  image: /assets/images/PAES_prediction_model/RandomForestBanner.jpg
  teaser: /assets/images/PAES_prediction_model/RandomForestBanner.jpg

categories:
  - Machine Learning
  - Aprendizaje Autom√°tico
  - Data Science
  - Ciencia de Datos
  - Educational Data Mining
  - Miner√≠a de Datos Educativos

tags:
  - Random Forest
  - Validaci√≥n Cruzada
  - Optimizaci√≥n de Modelos
  - Predicci√≥n de Resultados
  - PAES
  - Ense√±anza

---
<div align="justify" markdown="1">
Continuando nuestra exploraci√≥n en el mundo del an√°lisis de datos, nos enfrentamos ahora al desaf√≠o de afinar nuestro modelo de Random Forest. En este cap√≠tulo, nos centramos en entender mejor las decisiones detr√°s de nuestra programaci√≥n y el impacto que tienen en los resultados finales.

## üéì Comprendiendo el Porqu√© Detr√°s del C√≥digo üéì

Cada l√≠nea de c√≥digo en un an√°lisis de datos tiene un prop√≥sito y puede influir significativamente en el resultado final. Es esencial entender el 'porqu√©' detr√°s de cada funci√≥n y par√°metro para ser capaz de confiar y explicar los resultados de nuestro modelo.

Para no olvidar nuestro trabajo hasta ahora, te comparto el c√≥digo que dejamos en el cap√≠tulo 3:

<div align="center" markdown="1">

<a href="https://gist.github.com/daniavm/2b929e13e7438d3d40123a43149d40ff" target="_blank" align="center"> Revisar el c√≥digo aqu√≠</a>
</div>

A pesar de ser un c√≥digo que entrega resultados prometedores, a√∫n nos falta saber si es posible acercarnos a un nivel de confiabilidad superior y conciente. Para lograr esto √∫ltimo, comenzaremos entonces por analizar las piezas de c√≥digo que son fundamentales. 

### ¬øPor qu√© dividimos los datos?

```python
from sklearn.model_selection import train_test_split

# Dividimos los datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

La funci√≥n '***train_test_split***' es una herramienta que divide nuestros datos en dos partes: un conjunto para entrenar nuestro modelo y otro para probarlo. El test_size=0.2 significa que el 20% de los datos se reservan para probar el modelo, mientras que el resto se utiliza para el entrenamiento. 

Pero, ¬øpor qu√© 20% y no otro n√∫mero? Elegir el tama√±o del conjunto de prueba es una decisi√≥n que puede afectar la precisi√≥n de nuestras predicciones. Un conjunto de prueba demasiado peque√±o puede no capturar la variabilidad de los datos, mientras que uno demasiado grande podr√≠a no dejar suficientes datos para entrenar el modelo adecuadamente.

Esto √∫ltimo es la raz√≥n por la que este mecanismo de separaci√≥n de los datos puede no ser el mejor para generalizar nuestro modelo. 

### La Importancia de 'n_estimators' y 'random_state'

```python
from sklearn.ensemble import RandomForestRegressor

# Creamos el modelo de Random Forest
model = RandomForestRegressor(n_estimators=100, random_state=42)
```

El par√°metro ***n_estimators*** determina cu√°ntos √°rboles incluir en nuestro bosque. Cien √°rboles es un buen punto de partida, pero no hay una respuesta √∫nica para todos los casos. Es evidente que mientras m√°s √°rboles decidimos usar el modelo "podr√≠a mejorar" ya que la cantidad de posibles caminos considerados es mayor, pero tambi√©n requeriremos m√°s potencia de c√≥mputo. Debemos equilibrar la complejidad del modelo con la capacidad de generalizar bien a nuevos datos para as√≠ no usar recursos innecesarios y hacer que el c√≥digo sea tambi√©n √°gil.

El ***random_state*** es nuestra semilla de aleatoriedad. Utilizar un n√∫mero fijo, como 42, garantiza que si repetimos el an√°lisis obtendremos los mismos resultados cada vez que corremos el c√≥digo. Esto es crucial para la reproducibilidad de nuestro estudio y no sacar conclusiones diferentes por cada vez que vemos los resultados del output.


## üîç Refinando la Validaci√≥n del modelo: Superando la Divisi√≥n Est√°tica üîç

La divisi√≥n est√°ndar de los datos en un 80% para entrenamiento y un 20% para pruebas no siempre captura la complejidad y variabilidad inherentes en nuestro conjunto de datos. Para mejorar nuestra evaluaci√≥n del modelo y asegurarnos de que es robusto y confiable, implementamos una t√©cnica llamada "validaci√≥n cruzada".

```python
from sklearn.model_selection import cross_val_score, KFold

# Configuramos la validaci√≥n cruzada
kf = KFold(n_splits=5)
cross_val_scores = cross_val_score(model, X, y, cv=kf)
```

La funci√≥n '***KFold***' de '***sklearn***' nos permite dividir el conjunto de datos en m√∫ltiples segmentos o 'folds'. A diferencia de una √∫nica divisi√≥n de entrenamiento/prueba, la validaci√≥n cruzada eval√∫a el modelo en varias rondas, utilizando cada vez un segmento diferente como conjunto de prueba y el resto como entrenamiento. Esto garantiza que cada muestra de los datos se utilice tanto para entrenar como para validar el modelo, d√°ndonos una medida m√°s fiable de su rendimiento y evitando que ciertas peculiaridades de los datos influyan de manera desproporcionada en los resultados. En palabras simples, le da a todos los datos disponibles la posibilidad de ser parte del grupo de entrenamiento y tambi√©n del de prueba.

Esto consumir√° una cantidad de recursos de procesamiento mucho mayor, pero tambi√©n es una ganancia enorme en confiabilidad del modelo por lo que se sugiere muy fuertemente utilizar mecanismos de √©sta √≠ndole.

## üìê Analizando el Error Absoluto Medio (MAE) üìê

Una parte esencial en la afinaci√≥n de nuestro modelo Random Forest es la elecci√≥n de cu√°ntos 'folds' o particiones usar en la validaci√≥n cruzada. Esta decisi√≥n puede influir significativamente en la confiabilidad de las predicciones que hacemos. Para guiar esta elecci√≥n, recurrimos al concepto de Error Absoluto Medio (MAE), que nos ofrece una medida directa de cu√°nto se desv√≠an nuestras predicciones de los valores reales.

El MAE es la diferencia promedio entre el valor predicho y el valor real. En otras palabras, nos dice cu√°nto se equivoca nuestro modelo, en promedio, en las predicciones que hace. Un MAE bajo indica que nuestras predicciones son precisas, mientras que un MAE alto sugiere que podr√≠amos estar bastante lejos del objetivo.

### Explorando el n√∫mero √≥ptimo de 'folds'

Para encontrar el n√∫mero √≥ptimo de 'folds', realizamos un experimento iterando a trav√©s de un rango de valores y calculando el MAE para cada uno. Aqu√≠ est√° el fragmento de c√≥digo que lleva a cabo esta tarea:

```python
from sklearn.model_selection import cross_val_score, KFold
from sklearn.ensemble import RandomForestRegressor

# Experimento con diferentes cantidades de 'folds'
for fold in range(2, 80):
    # Configuramos el KFold con el n√∫mero actual de 'folds'
    kf = KFold(n_splits=fold)
    
    # Inicializamos el modelo Random Forest con 100 estimadores
    model = RandomForestRegressor(n_estimators=100)
    
    # Calculamos el MAE para el n√∫mero actual de 'folds'
    # Usamos la validaci√≥n cruzada para asegurarnos de que el c√°lculo del MAE es robusto
    mae_score = -cross_val_score(model, X, y, cv=kf, scoring='neg_mean_absolute_error').mean()
    
    # Imprimimos el n√∫mero de 'folds' y el MAE correspondiente
    print(f"{fold} folds: MAE = {mae_score}")
```

Cada iteraci√≥n del bucle **'for'** configura un nuevo objeto **'KFold'** con un n√∫mero diferente de 'folds' (que en este caso se mueve entre valores desde el 2 hasta el 80), que luego se utiliza para evaluar el modelo Random Forest. La funci√≥n cross_val_score se emplea aqu√≠ para realizar la validaci√≥n cruzada, y le pasamos el scoring parameter como '**neg_mean_absolute_error**' porque queremos calcular el MAE negativo; luego lo negamos (multiplicamos por -1) para convertirlo en un valor positivo que podemos interpretar f√°cilmente.

### Interpretando los Resultados con Visualizaciones

Tras realizar el experimento y calcular el MAE para cada n√∫mero de 'folds', es hora de interpretar los resultados. Una forma efectiva de hacerlo es a trav√©s de la visualizaci√≥n de datos. Los gr√°ficos nos permiten ver tendencias y patrones que pueden no ser evidentes solo con los n√∫meros.

El siguiente c√≥digo nos da una representaci√≥n gr√°fica de c√≥mo el MAE var√≠a con el n√∫mero de 'folds' utilizado en la validaci√≥n cruzada:

```python
import matplotlib.pyplot as plt
import seaborn as sns

# Gr√°fico de MAE promedio en funci√≥n del n√∫mero de folds
plt.figure(figsize=(10, 6))
sns.lineplot(x=num_folds_range, y=mae_scores_mean, marker='o')
plt.xlabel('N√∫mero de Folds de Validaci√≥n Cruzada')
plt.ylabel('Error Absoluto Medio (MAE) Promedio')
plt.title('An√°lisis del N√∫mero de Folds en Validaci√≥n Cruzada')
plt.grid()
plt.show()
```

El resultado de este experimento se muestra en el siguiente gr√°fico:

<figure style = "float: center; width: 100%; text-align: center; font-style: italic; font-size: 0.7em; text-indent: 0; margin: 0.6em; padding: 0.8em;">
  <a href="/assets/images/PAES_prediction_model/modelo_perdido_cap5_MAEvsFolds_n100.png">
    <img src="/assets/images/PAES_prediction_model/modelo_perdido_cap5_MAEvsFolds_n100.png" width="100%"  alt="Imagen 1: Resultado del experimento para ver c√≥mo cambian los resultados de MAE a medida que cambiamos la cantidad de Folds en el proceso de validaci√≥n cruzada.">
  </a>
  <figcaption>Imagen 1: Resultado del experimento para ver c√≥mo cambian los resultados de MAE a medida que cambiamos la cantidad de Folds en el proceso de validaci√≥n cruzada. En este caso, el eje Y est√° en t√©rminos de la cantidad de respuestas correctas en que el modelo predictivo difiere con los datos reales, mientras que el eje X representa la cantidad de KFolds considerados en cada iteraci√≥n.</figcaption>
</figure>

Como puedes ver, la precisi√≥n de nuestro modelo depende de los valores que tenga **'KFold'** y es de gran importancia determinar cu√°l es el valor adecuado para nuestro modelo predictivo y cu√°les fueron los criterios utilizados para definirlo. En nuestro caso, utilizar√© aquel KFold que minimice el valor de MAE, pero que tambien presente indicios de estabilidad en el modelo. Esto √∫ltimo se logra evidenciando sectores en que MAE sea constante dentro de este gr√°fico, o en palabras m√°s simples, cuando el MAE no cambie demasiado si yo  cambio KFolds.


## Ajustando el Enfoque: MAE, Folds y Estimadores

A medida que avanzamos en el refinamiento de nuestro modelo, nos damos cuenta de que la evaluaci√≥n del MAE no depende √∫nicamente de la cantidad de 'folds'. Hay otro factor en juego que puede ser igualmente importante: la cantidad de estimadores en nuestro Random Forest. De hecho me inquiet√≥ tanto el problema que hice un experimento similar al que hicimos con los 'folds' pero esta vez con el n√∫mero de estimadores. El resultado fue el siguiente gr√°fico:

<figure style = "float: center; width: 100%; text-align: center; font-style: italic; font-size: 0.7em; text-indent: 0; margin: 0.6em; padding: 0.8em;">
  <a href="/assets/images/PAES_prediction_model/modelo_perdido_cap5_MAEvsEstimators.png">
    <img src="/assets/images/PAES_prediction_model/modelo_perdido_cap5_MAEvsEstimators.png" width="100%"  alt="Imagen 2: Resultado del experimento para ver c√≥mo cambian los resultados de MAE a medida que cambiamos la cantidad de Estimadores en el proceso de validaci√≥n cruzada.">
  </a>
  <figcaption>Imagen 2: Resultado del experimento para ver c√≥mo cambian los resultados de MAE a medida que cambiamos la cantidad de Estimadores en el proceso de validaci√≥n cruzada.</figcaption>
</figure>

De llo que podemos ver, la precisi√≥n de las predicciones se ve claramente afectada por el n√∫mero de √°rboles que estamos utilizando para construir el modelo. Por lo tanto, nos enfrentamos a un an√°lisis tridimensional donde debemos considerar 'folds', estimadores y MAE simult√°neamente para optimizar nuestro modelo... y eso creo que puede ser el problema mas complejo de toda nuestra aventura üò®.

### Explorando la Interacci√≥n entre Folds y Estimadores

Para abordar esta complejidad, ampliamos nuestro experimento para incluir un rango de estimadores. Aqu√≠ est√° el c√≥digo que utilizamos para paralelizar el c√°lculo del MAE promedio, considerando ambos factores:

```python
from sklearn.model_selection import cross_val_score, KFold
from sklearn.ensemble import RandomForestRegressor
from joblib import Parallel, delayed
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Rangos de n√∫mero de estimadores y n√∫mero de folds a explorar
n_estimators_range = list(range(10, 1001, 10))
num_folds_range = list(range(2, 81, 1))

# Funci√≥n para calcular MAE promedio en paralelo para un n√∫mero de folds y estimadores
def calculate_mae_mean(num_folds, n_estimators):
    rf_regressor = RandomForestRegressor(n_estimators=n_estimators, random_state=42)
    kf = KFold(n_splits=num_folds)
    cv_mae_scores = -cross_val_score(rf_regressor, X, y, cv=kf, scoring='neg_mean_absolute_error')
    mae_mean = cv_mae_scores.mean()
    return mae_mean

# Paralelizar el c√°lculo del MAE promedio para diferentes n√∫meros de folds y estimadores
num_cores = 4  # Define el n√∫mero de n√∫cleos a utilizar en paralelo

mae_scores_mean = Parallel(n_jobs=num_cores)(
    delayed(calculate_mae_mean)(num_folds, n_estimators) 
    for num_folds in num_folds_range 
    for n_estimators in n_estimators_range
)

# Crear una matriz de valores de MAE en funci√≥n de num_folds_range y n_estimators_range
mae_matrix = np.array(mae_scores_mean).reshape(len(num_folds_range), -1)

# Crear el mapa de calor
plt.figure(figsize=(12, 8))
sns.heatmap(mae_matrix, cmap="YlOrRd", annot=False, fmt=".2f", xticklabels=n_estimators_range, yticklabels=num_folds_range)
plt.xlabel('N√∫mero de Estimadores (n_estimators)')
plt.ylabel('N√∫mero de Folds (num_folds_range)')
plt.title('Mapa de Calor de MAE en funci√≥n de Num Folds y Num Estimadores')
plt.show()
```

La raz√≥n por las que he parelelizado este algoritmo es simplemente porque tengo claro que es de una complejidad tan grande que soy consciente de que mi computador tardar√° bastante en evaluar algo as√≠. Estamos tomando 78 modelos distintos cuando cambiamos los 'folds' y 100 modelos extra por cada uno de estos donde probamos los distintos estimadores. Esto da un no menor total de 780 modelos de random forest utilizando validaci√≥n cruzada y calculando el MAE en cada iteraci√≥n ... as√≠ que tengo por seguro que mi querido Charlie (el nombre de mi computador) tendr√° una cuota de sufrimiento extra en nuestra aventura.

## ‚è≤Ô∏è 3 horas despu√©s ...

Despu√©s de harto maquinar el c√≥mo cumplir esta misi√≥n, el c√≥digo finalmente entreg√≥ su esperado resultado. Lo he puesto a continuaci√≥n para que tambi√©n puedan interactuar con √©l y saquen sus propias conclusiones hasta el siguiente cap√≠tulo. 

Supongo que ya ha sido suficiente por hoy ...


<figure style = "float: center; width: 100%; text-align: center; font-style: italic; font-size: 0.7em; text-indent: 0; margin: 0.6em; padding: 0.8em;">
  <a href="/assets/images/PAES_prediction_model/modelo_perdido_cap5_MAEvsEstimators.png">
    <embed type="text/html" src="assets/images/PAES_prediction_model/heatmap_interactivo.html" width="100%" height="200px" alt="Imagen 3: Heatmap en que se muestra el resultado del experimento en que se var√≠an la cantidad de folds y estimadores para calcular el MAE de los distintos modelos predictivos. ">
  </a>
  <figcaption>Imagen 3: Heatmap en que se muestra el resultado del experimento en que se var√≠an la cantidad de folds y estimadores para calcular el MAE de los distintos modelos predictivos.</figcaption>
</figure>


<div align="right" markdown="1">

_Hasta el pr√≥ximo cronopunto del Principia ü•ö._

DV

</div>

</div>