---
title: "En Busca del Modelo Perdido - Parte 5: Afinando el Random Forest"
layout: single
author_profile: true
related: true
comments: true
toc: true
toc_icon: "egg"

header:
  image: /assets/images/PAES_prediction_model/RandomForestComplex.jpg
  teaser: /assets/images/PAES_prediction_model/RandomForestComplex_teaser.jpg

categories:
  - Machine Learning
  - Aprendizaje Autom치tico
  - Data Science
  - Ciencia de Datos
  - Educational Data Mining
  - Miner칤a de Datos Educativos

tags:
  - Random Forest
  - Validaci칩n Cruzada
  - Optimizaci칩n de Modelos
  - Predicci칩n de Resultados
  - PAES
  - Ense침anza
---

<div align="justify" markdown="1">
Continuando nuestra exploraci칩n en el mundo del an치lisis de datos, nos enfrentamos ahora al desaf칤o de afinar nuestro modelo de Random Forest. En este cap칤tulo, nos centramos en entender mejor las decisiones detr치s de nuestra programaci칩n y el impacto que tienen en los resultados finales.

## 游꿉 Comprendiendo el Porqu칠 Detr치s del C칩digo 游꿉

Cada l칤nea de c칩digo en un an치lisis de datos tiene un prop칩sito y puede influir significativamente en el resultado final. Es esencial entender el 'porqu칠' detr치s de cada funci칩n y par치metro para ser capaz de confiar y explicar los resultados de nuestro modelo.

Para no olvidar nuestro trabajo hasta ahora, te comparto el c칩digo que dejamos en el cap칤tulo 3:

<div align="center" markdown="1">

<a href="https://gist.github.com/daniavm/2b929e13e7438d3d40123a43149d40ff" target="_blank" align="center"> Revisar el c칩digo aqu칤</a>
</div>

A pesar de ser un c칩digo que entrega resultados prometedores, a칰n nos falta saber si es posible acercarnos a un nivel de confiabilidad superior y conciente. Para lograr esto 칰ltimo, comenzaremos entonces por analizar las piezas de c칩digo que son fundamentales. 

### 쯇or qu칠 dividimos los datos?

```python
from sklearn.model_selection import train_test_split

# Dividimos los datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

La funci칩n '***train_test_split***' es una herramienta que divide nuestros datos en dos partes: un conjunto para entrenar nuestro modelo y otro para probarlo. El test_size=0.2 significa que el 20% de los datos se reservan para probar el modelo, mientras que el resto se utiliza para el entrenamiento. 

Pero, 쯣or qu칠 20% y no otro n칰mero? Elegir el tama침o del conjunto de prueba es una decisi칩n que puede afectar la precisi칩n de nuestras predicciones. Un conjunto de prueba demasiado peque침o puede no capturar la variabilidad de los datos, mientras que uno demasiado grande podr칤a no dejar suficientes datos para entrenar el modelo adecuadamente.

Esto 칰ltimo es la raz칩n por la que este mecanismo de separaci칩n de los datos puede no ser el mejor para generalizar nuestro modelo. 

### La Importancia de 'n_estimators' y 'random_state'

```python
from sklearn.ensemble import RandomForestRegressor

# Creamos el modelo de Random Forest
model = RandomForestRegressor(n_estimators=100, random_state=42)
```

El par치metro ***n_estimators*** determina cu치ntos 치rboles incluir en nuestro bosque. Cien 치rboles es un buen punto de partida, pero no hay una respuesta 칰nica para todos los casos. Es evidente que mientras m치s 치rboles decidimos usar el modelo "podr칤a mejorar" ya que la cantidad de posibles caminos considerados es mayor, pero tambi칠n requeriremos m치s potencia de c칩mputo. Debemos equilibrar la complejidad del modelo con la capacidad de generalizar bien a nuevos datos para as칤 no usar recursos innecesarios y hacer que el c칩digo sea tambi칠n 치gil.

El ***random_state*** es nuestra semilla de aleatoriedad. Utilizar un n칰mero fijo, como 42, garantiza que si repetimos el an치lisis obtendremos los mismos resultados cada vez que corremos el c칩digo. Esto es crucial para la reproducibilidad de nuestro estudio y no sacar conclusiones diferentes por cada vez que vemos los resultados del output.


## 游댌 Refinando la Validaci칩n del modelo: Superando la Divisi칩n Est치tica 游댌

La divisi칩n est치ndar de los datos en un 80% para entrenamiento y un 20% para pruebas no siempre captura la complejidad y variabilidad inherentes en nuestro conjunto de datos. Para mejorar nuestra evaluaci칩n del modelo y asegurarnos de que es robusto y confiable, implementamos una t칠cnica llamada "validaci칩n cruzada".

```python
from sklearn.model_selection import cross_val_score, KFold

# Configuramos la validaci칩n cruzada
kf = KFold(n_splits=5)
cross_val_scores = cross_val_score(model, X, y, cv=kf)
```

La funci칩n '***KFold***' de '***sklearn***' nos permite dividir el conjunto de datos en m칰ltiples segmentos o 'folds'. A diferencia de una 칰nica divisi칩n de entrenamiento/prueba, la validaci칩n cruzada eval칰a el modelo en varias rondas, utilizando cada vez un segmento diferente como conjunto de prueba y el resto como entrenamiento. Esto garantiza que cada muestra de los datos se utilice tanto para entrenar como para validar el modelo, d치ndonos una medida m치s fiable de su rendimiento y evitando que ciertas peculiaridades de los datos influyan de manera desproporcionada en los resultados. En palabras simples, le da a todos los datos disponibles la posibilidad de ser parte del grupo de entrenamiento y tambi칠n del de prueba.

Esto consumir치 una cantidad de recursos de procesamiento mucho mayor, pero tambi칠n es una ganancia enorme en confiabilidad del modelo por lo que se sugiere muy fuertemente utilizar mecanismos de 칠sta 칤ndole.

## 游늻 Analizando el Error Absoluto Medio (MAE) 游늻

Una parte esencial en la afinaci칩n de nuestro modelo Random Forest es la elecci칩n de cu치ntos 'folds' o particiones usar en la validaci칩n cruzada. Esta decisi칩n puede influir significativamente en la confiabilidad de las predicciones que hacemos. Para guiar esta elecci칩n, recurrimos al concepto de Error Absoluto Medio (MAE), que nos ofrece una medida directa de cu치nto se desv칤an nuestras predicciones de los valores reales.

El MAE es la diferencia promedio entre el valor predicho y el valor real. En otras palabras, nos dice cu치nto se equivoca nuestro modelo, en promedio, en las predicciones que hace. Un MAE bajo indica que nuestras predicciones son precisas, mientras que un MAE alto sugiere que podr칤amos estar bastante lejos del objetivo.

### Explorando el n칰mero 칩ptimo de 'folds'

Para encontrar el n칰mero 칩ptimo de 'folds', realizamos un experimento iterando a trav칠s de un rango de valores y calculando el MAE para cada uno. Aqu칤 est치 el fragmento de c칩digo que lleva a cabo esta tarea:

```python
from sklearn.model_selection import cross_val_score, KFold
from sklearn.ensemble import RandomForestRegressor

# Experimento con diferentes cantidades de 'folds'
for fold in range(2, 80):
    # Configuramos el KFold con el n칰mero actual de 'folds'
    kf = KFold(n_splits=fold)
    
    # Inicializamos el modelo Random Forest con 100 estimadores
    model = RandomForestRegressor(n_estimators=100)
    
    # Calculamos el MAE para el n칰mero actual de 'folds'
    # Usamos la validaci칩n cruzada para asegurarnos de que el c치lculo del MAE es robusto
    mae_score = -cross_val_score(model, X, y, cv=kf, scoring='neg_mean_absolute_error').mean()
    
    # Imprimimos el n칰mero de 'folds' y el MAE correspondiente
    print(f"{fold} folds: MAE = {mae_score}")
```

Cada iteraci칩n del bucle **'for'** configura un nuevo objeto **'KFold'** con un n칰mero diferente de 'folds' (que en este caso se mueve entre valores desde el 2 hasta el 80), que luego se utiliza para evaluar el modelo Random Forest. La funci칩n cross_val_score se emplea aqu칤 para realizar la validaci칩n cruzada, y le pasamos el scoring parameter como '**neg_mean_absolute_error**' porque queremos calcular el MAE negativo; luego lo negamos (multiplicamos por -1) para convertirlo en un valor positivo que podemos interpretar f치cilmente.

### Experimento con diferentes cantidades de estimadores y 'folds'
```python
for fold in range(2, 80):
    kf = KFold(n_splits=fold)
    model = RandomForestRegressor(n_estimators=100)
    mae_score = -cross_val_score(model, X, y, cv=kf, scoring='neg_mean_absolute_error').mean()
    print(f"{fold} folds: MAE = {mae_score}")
```

El bucle for en Python nos permite probar una serie de valores en un proceso iterativo, lo cual es ideal para experimentar con diferentes configuraciones de nuestro modelo. Aqu칤, estamos buscando el 'fold' 칩ptimo que minimice el Error Absoluto Medio (MAE), una medida de qu칠 tan lejos est치n nuestras predicciones de los valores reales.

### Interpretando los Resultados con Visualizaciones

Tras realizar el experimento y calcular el MAE para cada n칰mero de 'folds', es hora de interpretar los resultados. Una forma efectiva de hacerlo es a trav칠s de la visualizaci칩n de datos. Los gr치ficos nos permiten ver tendencias y patrones que pueden no ser evidentes solo con los n칰meros.

El siguiente c칩digo nos da una representaci칩n gr치fica de c칩mo el MAE var칤a con el n칰mero de 'folds' utilizado en la validaci칩n cruzada:

```python
import matplotlib.pyplot as plt
import seaborn as sns

# Gr치fico de MAE promedio en funci칩n del n칰mero de folds
plt.figure(figsize=(10, 6))
sns.lineplot(x=num_folds_range, y=mae_scores_mean, marker='o')
plt.xlabel('N칰mero de Folds de Validaci칩n Cruzada')
plt.ylabel('Error Absoluto Medio (MAE) Promedio')
plt.title('An치lisis del N칰mero de Folds en Validaci칩n Cruzada')
plt.grid()
plt.show()
```

El resultado de este experimento se muestra en el siguiente gr치fico:

<figure style = "float: center; width: 100%; text-align: center; font-style: italic; font-size: 0.7em; text-indent: 0; margin: 0.6em; padding: 0.8em;">
  <a href="/assets/images/PAES_prediction_model/modelo_perdido_cap5_MAEvsFolds_n100.png">
    <img src="/assets/images/PAES_prediction_model/modelo_perdido_cap5_MAEvsFolds_n100.png" width="100%"  alt="Imagen 1: Resultado del experimento para ver c칩mo cambian los resultados de MAE a medida que cambiamos la cantidad de Folds en el proceso de validaci칩n cruzada.">
  </a>
  <figcaption>Imagen 1: Resultado del experimento para ver c칩mo cambian los resultados de MAE a medida que cambiamos la cantidad de Folds en el proceso de validaci칩n cruzada. En este caso, el eje Y est치 en t칠rminos de la cantidad de respuestas correctas en que el modelo predictivo difiere con los datos reales, mientras que el eje X representa la cantidad de KFolds considerados en cada iteraci칩n.</figcaption>
</figure>

Como puedes ver, la precisi칩n de nuestro modelo depende de los valores que tenga **'KFold'** y es de gran importancia determinar cu치l es el valor adecuado para nuestro modelo predictivo y cu치les fueron los criterios utilizados para definirlo. En nuestro caso, utilizar칠 aquel KFold que minimice el valor de MAE, pero que tambien presente indicios de estabilidad en el modelo. Esto 칰ltimo se logra evidenciando sectores en que MAE sea constante dentro de este gr치fico, o en palabras m치s simples, cuando el MAE no cambie demasiado si yo  cambio KFolds.


## Ajustando el Enfoque: MAE, Folds y Estimadores

A medida que avanzamos en el refinamiento de nuestro modelo, nos damos cuenta de que la evaluaci칩n del MAE no depende 칰nicamente de la cantidad de 'folds'. Hay otro factor en juego que puede ser igualmente importante: la cantidad de estimadores en nuestro Random Forest. La precisi칩n de las predicciones podr칤a verse afectada por el n칰mero de 치rboles que estamos utilizando para construir el modelo. Por lo tanto, nos enfrentamos a un an치lisis tridimensional donde debemos considerar 'folds', estimadores y MAE simult치neamente para optimizar nuestro modelo.

### Explorando la Interacci칩n entre Folds y Estimadores

Para abordar esta complejidad, ampliamos nuestro experimento para incluir un rango de estimadores. Aqu칤 est치 el c칩digo que utilizamos para paralelizar el c치lculo del MAE promedio, considerando ambos factores:

```python
from sklearn.model_selection import cross_val_score, KFold
from sklearn.ensemble import RandomForestRegressor
from joblib import Parallel, delayed
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Rangos de n칰mero de estimadores y n칰mero de folds a explorar
n_estimators_range = list(range(10, 1001, 10))
num_folds_range = list(range(2, 81, 1))

# Funci칩n para calcular MAE promedio en paralelo para un n칰mero de folds y estimadores
def calculate_mae_mean(num_folds, n_estimators):
    rf_regressor = RandomForestRegressor(n_estimators=n_estimators, random_state=42)
    kf = KFold(n_splits=num_folds)
    cv_mae_scores = -cross_val_score(rf_regressor, X, y, cv=kf, scoring='neg_mean_absolute_error')
    mae_mean = cv_mae_scores.mean()
    return mae_mean

# Paralelizar el c치lculo del MAE promedio para diferentes n칰meros de folds y estimadores
num_cores = 4  # Define el n칰mero de n칰cleos a utilizar en paralelo

mae_scores_mean = Parallel(n_jobs=num_cores)(
    delayed(calculate_mae_mean)(num_folds, n_estimators) 
    for num_folds in num_folds_range 
    for n_estimators in n_estimators_range
)

# Crear una matriz de valores de MAE en funci칩n de num_folds_range y n_estimators_range
mae_matrix = np.array(mae_scores_mean).reshape(len(num_folds_range), -1)

# Crear el mapa de calor
plt.figure(figsize=(12, 8))
sns.heatmap(mae_matrix, cmap="YlOrRd", annot=False, fmt=".2f", xticklabels=n_estimators_range, yticklabels=num_folds_range)
plt.xlabel('N칰mero de Estimadores (n_estimators)')
plt.ylabel('N칰mero de Folds (num_folds_range)')
plt.title('Mapa de Calor de MAE en funci칩n de Num Folds y Num Estimadores')
plt.show()
```




Hasta entonces, nos vemos en el pr칩ximo cronopunto del Principia 游볰.

DV

</div>