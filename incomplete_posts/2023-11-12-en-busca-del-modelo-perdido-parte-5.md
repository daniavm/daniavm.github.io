---
title: "En Busca del Modelo Perdido - Parte 5: Afinando el Random Forest"
layout: single
author_profile: true
related: true
comments: true
toc: false

header:
  image: /assets/images/PAES_prediction_model/RandomForestComplex.jpg
  teaser: /assets/images/PAES_prediction_model/RandomForestComplex_teaser.jpg

categories:
  - Machine Learning
  - Aprendizaje Autom치tico
  - Data Science
  - Ciencia de Datos
  - Educational Data Mining
  - Miner칤a de Datos Educativos

tags:
  - Random Forest
  - Validaci칩n Cruzada
  - Optimizaci칩n de Modelos
  - Predicci칩n de Resultados
  - PAES
  - Ense침anza
---

<div align="justify" markdown="1">
Continuando nuestra exploraci칩n en el mundo del an치lisis de datos, nos enfrentamos ahora al desaf칤o de afinar nuestro modelo de Random Forest. En este cap칤tulo, nos centramos en entender mejor las decisiones detr치s de nuestra programaci칩n y el impacto que tienen en los resultados finales.

## 游꿉 Comprendiendo el Porqu칠 Detr치s del C칩digo 游꿉

Cada l칤nea de c칩digo en un an치lisis de datos tiene un prop칩sito y puede influir significativamente en el resultado final. Es esencial entender el 'porqu칠' detr치s de cada funci칩n y par치metro para ser capaz de confiar y explicar los resultados de nuestro modelo.

Para no olvidar nuestro trabajo hasta ahora, te comparto el c칩digo que dejamos en el cap칤tulo 3:

<div align="center" markdown="1">
[Revisar el c칩digo aqu칤](https://gist.github.com/daniavm/2b929e13e7438d3d40123a43149d40ff.js)
</div>

A pesar de ser un c칩digo que entrega resultados prometedores, a칰n nos falta saber si es posible acercarnos a un nivel de confiabilidad superior y conciente. Para lograr esto 칰ltimo, comenzaremos entonces por analizar las piezas de c칩digo que son fundamentales. 

### 쯇or qu칠 dividimos los datos?

```python
from sklearn.model_selection import train_test_split

# Dividimos los datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

La funci칩n '***train_test_split***' es una herramienta que divide nuestros datos en dos partes: un conjunto para entrenar nuestro modelo y otro para probarlo. El test_size=0.2 significa que el 20% de los datos se reservan para probar el modelo, mientras que el resto se utiliza para el entrenamiento. Pero, 쯣or qu칠 20% y no otro n칰mero? Elegir el tama침o del conjunto de prueba es una decisi칩n que puede afectar la precisi칩n de nuestras predicciones. Un conjunto de prueba demasiado peque침o puede no capturar la variabilidad de los datos, mientras que uno demasiado grande podr칤a no dejar suficientes datos para entrenar el modelo adecuadamente.

### La Importancia de 'n_estimators' y 'random_state'

```python
from sklearn.ensemble import RandomForestRegressor

# Creamos el modelo de Random Forest
model = RandomForestRegressor(n_estimators=100, random_state=42)
```

El par치metro ***n_estimators*** determina cu치ntos 치rboles incluir en nuestro bosque. Cien 치rboles es un buen punto de partida, pero no hay una respuesta 칰nica para todos los casos. Necesitamos equilibrar la complejidad del modelo con la capacidad de generalizar bien a nuevos datos.

El ***random_state*** es nuestra semilla de aleatoriedad. Utilizar un n칰mero fijo, como 42, garantiza que si repetimos el an치lisis obtendremos los mismos resultados. Esto es crucial para la reproducibilidad de nuestro estudio.


## 游댌 Refinando la Validaci칩n del modelo: Superando la Divisi칩n Est치tica 游댌

La divisi칩n est치ndar de los datos en un 80% para entrenamiento y un 20% para pruebas no siempre captura la complejidad y variabilidad inherentes en nuestro conjunto de datos. Para mejorar nuestra evaluaci칩n del modelo y asegurarnos de que es robusto y confiable, implementamos la validaci칩n cruzada.

```python
from sklearn.model_selection import cross_val_score, KFold

# Configuramos la validaci칩n cruzada
kf = KFold(n_splits=5)
cross_val_scores = cross_val_score(model, X, y, cv=kf)
```

La funci칩n '***KFold***' de '***sklearn***' nos permite dividir el conjunto de datos en m칰ltiples segmentos o 'folds'. A diferencia de una 칰nica divisi칩n de entrenamiento/prueba, la validaci칩n cruzada eval칰a el modelo en varias rondas, utilizando cada vez un segmento diferente como conjunto de prueba y el resto como entrenamiento. Esto garantiza que cada muestra de los datos se utilice tanto para entrenar como para validar el modelo, d치ndonos una medida m치s fiable de su rendimiento y evitando que ciertas peculiaridades de los datos influyan de manera desproporcionada en los resultados.

## 游늻 Analizando el Error Absoluto Medio (MAE) 游늻

Una parte esencial en la afinaci칩n de nuestro modelo Random Forest es la elecci칩n de cu치ntos 'folds' o particiones usar en la validaci칩n cruzada. Esta decisi칩n puede influir significativamente en la confiabilidad de las predicciones que hacemos. Para guiar esta elecci칩n, recurrimos al Error Absoluto Medio (MAE), que nos ofrece una medida directa de cu치nto se desv칤an nuestras predicciones de los valores reales.

El MAE es la diferencia promedio entre el valor predicho y el valor real. En otras palabras, nos dice cu치nto se equivoca nuestro modelo, en promedio, en las predicciones que hace. Un MAE bajo indica que nuestras predicciones son precisas, mientras que un MAE alto sugiere que podr칤amos estar bastante lejos del objetivo.

### Explorando el n칰mero 칩ptimo de 'folds'

Para encontrar el n칰mero 칩ptimo de 'folds', realizamos un experimento iterando a trav칠s de un rango de valores y calculando el MAE para cada uno. Aqu칤 est치 el fragmento de c칩digo que lleva a cabo esta tarea:

```python
from sklearn.model_selection import cross_val_score, KFold
from sklearn.ensemble import RandomForestRegressor

# Experimento con diferentes cantidades de 'folds'
for fold in range(2, 80):
    # Configuramos el KFold con el n칰mero actual de 'folds'
    kf = KFold(n_splits=fold)
    
    # Inicializamos el modelo Random Forest con 100 estimadores
    model = RandomForestRegressor(n_estimators=100)
    
    # Calculamos el MAE para el n칰mero actual de 'folds'
    # Usamos la validaci칩n cruzada para asegurarnos de que el c치lculo del MAE es robusto
    mae_score = -cross_val_score(model, X, y, cv=kf, scoring='neg_mean_absolute_error').mean()
    
    # Imprimimos el n칰mero de 'folds' y el MAE correspondiente
    print(f"{fold} folds: MAE = {mae_score}")
```

Cada iteraci칩n del bucle **'for'** configura un nuevo objeto **'KFold'** con un n칰mero diferente de 'folds', que luego se utiliza para evaluar el modelo Random Forest. La funci칩n cross_val_score se emplea aqu칤 para realizar la validaci칩n cruzada, y le pasamos el scoring parameter como '**neg_mean_absolute_error**' porque queremos calcular el MAE negativo; lo negamos (multiplicamos por -1) para convertirlo en un valor positivo que podemos interpretar f치cilmente.

### Interpretando los Resultados con Visualizaciones

Tras realizar el experimento y calcular el MAE para cada n칰mero de 'folds', es hora de interpretar los resultados. Una forma efectiva de hacerlo es a trav칠s de la visualizaci칩n de datos. Los gr치ficos nos permiten ver tendencias y patrones que pueden no ser evidentes solo con los n칰meros.

#### Visualizaci칩n del MAE y el N칰mero de Folds

El siguiente c칩digo nos da una representaci칩n gr치fica de c칩mo el MAE var칤a con el n칰mero de 'folds' utilizado en la validaci칩n cruzada:

```python
import matplotlib.pyplot as plt
import seaborn as sns

# Gr치fico de MAE promedio en funci칩n del n칰mero de folds
plt.figure(figsize=(10, 6))
sns.lineplot(x=num_folds_range, y=mae_scores_mean, marker='o')
plt.xlabel('N칰mero de Folds de Validaci칩n Cruzada')
plt.ylabel('Error Absoluto Medio (MAE) Promedio')
plt.title('An치lisis del N칰mero de Folds en Validaci칩n Cruzada (Paralelizado)')
plt.grid()
plt.show()
```

En este gr치fico de l칤neas, cada punto representa el MAE promedio para un n칰mero espec칤fico de 'folds'. Lo que buscamos es una l칤nea que tienda a estabilizarse, indicando que hemos alcanzado un punto en el que aumentar el n칰mero de 'folds' no mejora significativamente el MAE. Encontrar este punto de equilibrio nos ayuda a evitar el sobreajuste y el subajuste, garantizando que nuestro modelo sea generalizable.

# Experimento con diferentes cantidades de estimadores y 'folds'
for fold in range(2, 80):
    kf = KFold(n_splits=fold)
    model = RandomForestRegressor(n_estimators=100)
    mae_score = -cross_val_score(model, X, y, cv=kf, scoring='neg_mean_absolute_error').mean()
    print(f"{fold} folds: MAE = {mae_score}")


El bucle for en Python nos permite probar una serie de valores en un proceso iterativo, lo cual es ideal para experimentar con diferentes configuraciones de nuestro modelo. Aqu칤, estamos buscando el 'fold' 칩ptimo que minimice el Error Absoluto Medio (MAE), una medida de qu칠 tan lejos est치n nuestras predicciones de los valores reales.

## El Viaje hacia un Modelo M치s Preciso
Al final, lo que buscamos es un modelo que nos ofrezca predicciones confiables. Para llegar a esto, hemos refinado nuestro Random Forest y ahora miramos hacia los resultados reales.

# Visualizaci칩n de los resultados del modelo
plt.figure(figsize=(10, 6))
plt.scatter(y, y_pred, alpha=0.5)
plt.xlabel('Valores Reales de PAES')
plt.ylabel('Predicciones de PAES')
plt.title('Comparaci칩n de Valores Reales y Predicciones')
plt.grid()
plt.show()


Esta visualizaci칩n es crucial. Al comparar los valores reales y las predicciones con un gr치fico de dispersi칩n, podemos ver visualmente la precisi칩n de nuestro modelo. Si los puntos se alinean cerca de una l칤nea imaginaria diagonal, nuestro modelo est치 en buen camino.

## 游꿉 Lecciones Aprendidas y Camino a Seguir 游꿉
Este cap칤tulo nos ha llevado a trav칠s de una exploraci칩n detallada de c칩mo ajustar nuestro modelo de Random Forest. Hemos aprendido que cada par치metro cuenta su propia historia y que solo a trav칠s de la comprensi칩n y la experimentaci칩n podemos esperar acercarnos a la verdad detr치s de nuestros datos.

En la pr칩xima etapa de nuestro viaje, continuaremos afinando nuestro modelo, siempre con la mente abierta y dispuestos a aprender de los datos que tenemos entre manos.

Hasta entonces, nos vemos en el pr칩ximo cronopunto del Principia 游볰.

DV

</div>