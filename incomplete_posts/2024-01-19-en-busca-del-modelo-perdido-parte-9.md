---
title: "En Busca del Modelo Perdido - Parte 9: Navegando el Laberinto de Hiperpar치metros de XGBoost"
layout: single
author_profile: false
related: true
comments: true
toc: false
date: 2024-01-19T11:48:41-04:00

sidebar:
  nav: "modelo_perdido"

classes: wide

header:
  image: /assets/images/PAES_prediction_model/busca_del_modelo_perdido_parte_8.png
  teaser: /assets/images/PAES_prediction_model/busca_del_modelo_perdido_parte_8.png

categories:
  - Machine Learning

tags:
  - XGBoost
  - Hiperpar치metros
  - Sobreajuste
  - Validaci칩n Cruzada
  - Predicci칩n de PAES
  - Aprendizaje Autom치tico
  - An치lisis de Datos
  - T칠cnicas de Boosting

---
<div align="justify" markdown="1">
En nuestra inquebrantable b칰squeda del modelo perfecto para predecir los resultados de la prueba PAES, nos enfrentamos a uno de los desaf칤os m치s complejos y enigm치ticos: la optimizaci칩n de hiperpar치metros en XGBoost. Este cap칤tulo nos sumerge en el laberinto multidimensional de hiperpar치metros, donde la mente humana lucha por visualizar la soluci칩n debido a la complejidad matem치tica que subyace en este problema.

## Hiperpar치metros: El Laberinto Multidimensional

Imagina que te encuentras en un terreno monta침oso, buscando el punto m치s bajo, el "m칤nimo", en este terreno irregular. Cada metro cuadrado representa una combinaci칩n 칰nica de hiperpar치metros que afectan el rendimiento de tu modelo.

Al principio, est치s en un mundo bidimensional, como un plano extenso. En este mundo, encontrar un m칤nimo es relativamente sencillo, pero es como explorar un corte de pastel sin saber si es el 칰nico.

Luego avanzas a un mundo tridimensional, donde cortas el terreno en diferentes direcciones, pero a칰n no puedes visualizar el paisaje completo. Aqu칤, encontrar un m칤nimo local se convierte en un desaf칤o. Las curvas de nivel representan los resultados del MAE en tu b칰squeda, midiendo cu치n cerca est치s del m칤nimo absoluto. El objetivo es minimizar el MAE, que refleja cu치n preciso es tu modelo.

Sin embargo, el MAE puede ser enga침oso, ya que no garantiza el m칤nimo absoluto. Aqu칤 es donde entra el desaf칤o: buscar el valor m칤nimo de una funci칩n topol칩gica en un espacio multidimensional. El MAE se convierte en una gu칤a para explorar las curvas de nivel y acercarse al m칤nimo.

Finalmente, te adentras en un mundo multidimensional, donde cada hiperpar치metro es una dimensi칩n adicional. Visualizar esto se vuelve abrumador. Es como cortar el terreno en diferentes direcciones, niveles y 치ngulos en todas las dimensiones posibles.

Aqu칤 nuestra mente humana se queda atr치s, incapaz de visualizar el espacio completo sin recurrir a las matem치ticas o a la "fuerza bruta" de probar innumerables combinaciones de hiperpar치metros.

Una idea de lo que he expresado aqu칤 se muestra en el siguiente gr치fico para poder ayudar a la idea de la b칰squeda de un m칤nimo en un espacio dado (que en este caso es tridimensional).

<figure style="width: 100%; text-align: center;">
    <embed type="text/html" src="/assets/images/simple_post_images/minimos_superficie.html" style="width: 100%; height: 500px; border: none;" alt="Imagen 1: Heatmap del An치lisis de MAE.">
    <figcaption>Imagen 1: Ilustraci칩n del Desaf칤o de la B칰squeda de M칤nimos - Esta visualizaci칩n en 3D representa un terreno complejo que simboliza el desaf칤o de encontrar el m칤nimo absoluto en el contexto de los hiperpar치metros. Los cortes verticales azules muestran diferentes perspectivas de este terreno multidimensional, revelando m칰ltiples m칤nimos locales dependiendo del corte elegido. En este laberinto de posibilidades, explorar cada punto ser칤a una tarea costosa en t칠rminos de tiempo y recursos. Es aqu칤 donde entra en juego el "gradiente," representado por la l칤nea roja, que gu칤a la b칰squeda hacia un m칤nimo local desde un punto de partida espec칤fico. Sin embargo, el uso del gradiente puede llevarnos a m칤nimos locales en lugar del m칤nimo absoluto, lo que ilustra la complejidad de encontrar la combinaci칩n 칩ptima de hiperpar치metros para nuestro modelo.</figcaption>
</figure>

En esta traves칤a, nos encontramos en la encrucijada de elegir entre m칠todos que nos ayudar치n a navegar este laberinto de manera eficiente. La elecci칩n de la herramienta adecuada es fundamental, ya que nos permitir치 explorar este laberinto multidimensional con inteligencia y nos acercar치 cada vez m치s a encontrar el modelo perdido que buscamos.


## Comparando Grid Search y Hyperopt
Ahora que hemos desvelado los tesoros de nuestros hiperpar치metros y comprendido su funcionamiento (cap칤tulo anterior), es hora de aprender c칩mo encontrar las combinaciones 칩ptimas para ajustar nuestro modelo XGBoost. En esta secci칩n, compararemos dos enfoques populares para optimizar los hiperpar치metros: Grid Search y Hyperopt.

### Grid Search: Navegando el Laberinto de Hiperpar치metros

Comencemos con Grid Search, un m칠todo que se asemeja a tener un mapa detallado pero limitado de nuestro laberinto de hiperpar치metros. Este enfoque es bastante simple de entender y usar, pero puede ser lento en espacios de b칰squeda extensos.

Grid Search es como explorar el espacio de hiperpar치metros siguiendo cortes precisos a trav칠s de la superficie que vimos en el gr치fico interactivo. Similar a c칩mo los cortes azules en el gr치fico representan diferentes combinaciones de hiperpar치metros, Grid Search prueba todas las combinaciones posibles dentro de un espacio de b칰squeda predefinido.

```python
from sklearn.model_selection import GridSearchCV
from xgboost import XGBRegressor

# Define el espacio de b칰squeda de hiperpar치metros
param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5]
}

# Crea un modelo base
model = XGBRegressor(random_state=42)

# Inicializa Grid Search
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, 
                           scoring='neg_mean_absolute_error', cv=5)

# Ajusta Grid Search a tus datos
grid_search.fit(X_train, y_train)

# Obtiene los mejores hiperpar치metros
best_params = grid_search.best_params_
best_mae = -grid_search.best_score_

print(f"Mejores hiperpar치metros encontrados: {best_params}")
print(f"Mejor puntuaci칩n MAE: {best_mae}")
```

### Hyperopt: Exploraci칩n Inteligente del Laberinto

Ahora, cambiemos nuestro enfoque hacia Hyperopt, un m칠todo que se asemeja a un explorador inteligente en nuestro laberinto. Hyperopt utiliza algoritmos para buscar de manera m치s eficiente en el espacio de hiperpar치metros. Esto lo convierte en una excelente opci칩n cuando el espacio de b칰squeda es extenso y deseas una optimizaci칩n m치s r치pida.

Hyperopt es como seguir el gradiente en el gr치fico interactivo que presentamos. El gradiente representa la direcci칩n 칩ptima para buscar la combinaci칩n perfecta de hiperpar치metros que minimice el MAE. En lugar de probar todas las combinaciones, Hyperopt ajusta sus pasos para avanzar de manera m치s efectiva hacia el m칤nimo.

```python
from hyperopt import hp, fmin, tpe, Trials, STATUS_OK, space_eval
from xgboost import XGBRegressor

# Define el espacio de b칰squeda de hiperpar치metros
space = {
    'n_estimators': hp.choice('n_estimators', [100, 200, 300]),
    'learning_rate': hp.loguniform('learning_rate', -3, 0),
    'max_depth': hp.quniform('max_depth', 3, 5, 1)
}

# Funci칩n objetivo para Hyperopt
def objective(params):
    model = XGBRegressor(random_state=42, n_jobs=1, **params)
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    mae = -np.mean(cross_val_score(model, X_scaled, y, cv=kf, scoring='neg_mean_absolute_error'))
    return {'loss': mae, 'status': STATUS_OK}

# Inicializa Trials para el registro
trials = Trials()

# Ejecuta Hyperopt para la optimizaci칩n
best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=N_TRIALS, trials=trials)

# Obtiene los mejores hiperpar치metros y MAE
best_params = space_eval(space, best)
best_mae = -min(trials.losses())

print(f"Mejores hiperpar치metros encontrados: {best_params}")
print(f"Mejor puntuaci칩n MAE: {best_mae}")
```

## Una Elecci칩n Importante: Definir el Espacio de B칰squeda

Es importante destacar que la elecci칩n del espacio de b칰squeda de hiperpar치metros es crucial para el 칠xito de estas t칠cnicas ("param_grid" en el caso de Grid Search y "space" en el caso de Hyperopt). Tanto Grid Search como Hyperopt pueden buscar eficientemente dentro de los l칤mites que establezcamos. Por lo tanto, definir adecuadamente estos l칤mites requiere un conocimiento s칩lido del problema y de las caracter칤sticas estad칤sticas de los hiperpar치metros.

Ambos m칠todos tienen sus ventajas y desventajas, y la elecci칩n entre ellos depender치 de la complejidad del espacio de b칰squeda y de tus preferencias personales. Grid Search es simple y exhaustivo, pero puede ser lento en espacios de b칰squeda grandes. Hyperopt es m치s eficiente pero requiere m치s configuraci칩n. Es an치logo al dilema de recorrer la superficie de b칰squeda punto por punto y saber que vamos a encontrar la soluci칩n al punto m치s bajo, o bien decidirse a ir en busca de este punto solo utilizando herramientas locales como la visi칩n del terreno, la gravedad o cualquier otro para decir direccionar nuestra caminata al punto "mas bajo".

La Elecci칩n de Hyperopt

Dada la complejidad de nuestro laberinto de hiperpar치metros y la necesidad de una soluci칩n eficiente, hemos elegido Hyperopt como nuestra herramienta preferida. Su enfoque basado en la optimizaci칩n bayesiana nos permite navegar este terreno con mayor precisi칩n y rapidez. En los pr칩ximos pasos de nuestra aventura, profundizaremos en el uso de Hyperopt y descubriremos c칩mo esta elecci칩n nos acerca cada vez m치s a encontrar el modelo perdido que buscamos.

<div align="right" markdown="1">
Hasta el pr칩ximo cronopunto del Principia 游볰.

DV

</div>
</div>