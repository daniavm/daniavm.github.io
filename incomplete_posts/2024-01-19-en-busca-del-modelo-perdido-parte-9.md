---
title: "En Busca del Modelo Perdido - Parte 9: Navegando el Laberinto de Hiperpar치metros de XGBoost"
layout: single
author_profile: false
related: true
comments: true
toc: false
date: 2024-01-19T11:48:41-04:00

sidebar:
  nav: "modelo_perdido"

classes: wide

header:
  image: /assets/images/PAES_prediction_model/busca_del_modelo_perdido_parte_8.png
  teaser: /assets/images/PAES_prediction_model/busca_del_modelo_perdido_parte_8.png

categories:
  - Machine Learning

tags:
  - XGBoost
  - Hiperpar치metros
  - Sobreajuste
  - Validaci칩n Cruzada
  - Predicci칩n de PAES
  - Aprendizaje Autom치tico
  - An치lisis de Datos
  - T칠cnicas de Boosting

---
<div align="justify" markdown="1">
En nuestra inquebrantable b칰squeda del modelo perfecto para predecir los resultados de la prueba PAES, nos enfrentamos a uno de los desaf칤os m치s complejos y enigm치ticos: la optimizaci칩n de hiperpar치metros en XGBoost. Este cap칤tulo nos sumerge en el laberinto multidimensional de hiperpar치metros, donde la mente humana lucha por visualizar la soluci칩n debido a la complejidad matem치tica que subyace en este problema.

## Hiperpar치metros: El Laberinto Multidimensional

Imagina que te encuentras en un terreno monta침oso, buscando el punto m치s bajo, el "m칤nimo", en este terreno irregular. Cada metro cuadrado representa una combinaci칩n 칰nica de hiperpar치metros que influyen en el rendimiento de tu modelo. Al principio, est치s en un mundo bidimensional, como un plano extenso. En este mundo, encontrar un m칤nimo es relativamente sencillo, pero es como explorar un corte de pastel sin saber si es el 칰nico.

Avanzas a un mundo tridimensional, donde cortas el terreno en diferentes direcciones, pero a칰n no puedes visualizar el paisaje completo. Esto representa el desaf칤o de encontrar un m칤nimo local. Las curvas de nivel que ves son como los resultados de MAE en tu b칰squeda, que miden cu치n cerca est치s del m칤nimo absoluto. El objetivo es minimizar el MAE, que representa la diferencia promedio entre las predicciones del modelo y los valores reales. Cuanto m치s bajo sea el MAE, m치s preciso ser치 tu modelo.

Sin embargo, el MAE puede ser enga침oso, ya que no garantiza el m칤nimo absoluto. Es como buscar el valor m칤nimo de una funci칩n topol칩gica en un espacio multidimensional, donde el MAE se convierte en una gu칤a para explorar las curvas de nivel y acercarse al m칤nimo.

Luego, te adentras en un mundo multidimensional, donde cada dimensi칩n adicional representa un hiperpar치metro diferente. Visualizar esto se vuelve abrumador. Es como si cortaras el terreno en diferentes direcciones, niveles y 치ngulos en todas las dimensiones posibles. Aqu칤 es donde nuestra mente humana se queda atr치s, incapaz de visualizar el espacio completo sin recurrir a las matem치ticas o a la "fuerza bruta" de probar innumerables combinaciones de hiperpar치metros.

Cada hiperpar치metro es una dimensi칩n adicional en este mundo, y encontrar el m칤nimo absoluto del MAE se convierte en un desaf칤o que va m치s all치 de la intuici칩n humana. Es como buscar la combinaci칩n perfecta de hiperpar치metros que minimice el error promedio entre las predicciones y los valores reales, lo que nos acerca cada vez m치s a encontrar el modelo perdido que buscamos.

En esta traves칤a, nos encontramos en la encrucijada de elegir entre m칠todos que nos ayudar치n a navegar este laberinto de manera eficiente. La elecci칩n de la herramienta adecuada es fundamental, ya que nos permitir치 explorar este laberinto multidimensional con inteligencia y nos acercar치 cada vez m치s a encontrar el modelo perdido que buscamos.

<figure style="width: 100%; text-align: center;">
    <embed type="text/html" src="/assets/images/simple_post_images/minimos_superficie.html" style="width: 100%; height: 500px; border: none;" alt="Imagen 1: Heatmap del An치lisis de MAE.">
    <figcaption>Imagen 1 (visible en computadores): Heatmap interactivo que muestra c칩mo var칤an los valores de MAE con diferentes combinaciones de 'n_estimators' y 'num_folds'.</figcaption>
</figure>


Los Tesoros Escondidos: Nuestros Hiperpar치metros

Dentro de este laberinto multidimensional, destacan los siguientes tesoros escondidos, que representan los hiperpar치metros clave utilizados en nuestro c칩digo:

Learning Rate (Tasa de Aprendizaje): Controla la velocidad de ajuste del modelo a los datos.
N_Estimators (N칰mero de Estimadores): Determina la cantidad de 치rboles en nuestro modelo.
Gamma: Act칰a como un regulador de la complejidad del modelo.
Min Child Weight: Define un umbral para la divisi칩n de nodos.
Subsample: Controla la fracci칩n de muestras utilizadas para entrenar cada 치rbol.
Colsample_by:* Regula la selecci칩n de caracter칤sticas.
Comparando Grid Search y Hyperopt

Para descifrar este laberinto de hiperpar치metros, tenemos dos herramientas a nuestra disposici칩n: Grid Search y Hyperopt. Ambos m칠todos buscan la combinaci칩n 칩ptima de hiperpar치metros, pero difieren en su enfoque y eficiencia.

Grid Search es como explorar sistem치ticamente cada rinc칩n del laberinto, probando todas las combinaciones posibles de hiperpar치metros. Es exhaustivo pero puede ser ineficiente y requerir una gran cantidad de tiempo y recursos computacionales.

Hyperopt, por otro lado, es como un explorador inteligente que aprende de cada paso y ajusta su camino en consecuencia. Utiliza algoritmos de optimizaci칩n bayesiana para encontrar la combinaci칩n 칩ptima de hiperpar치metros de manera m치s eficiente y efectiva.

La Elecci칩n de Hyperopt

Dada la complejidad de nuestro laberinto de hiperpar치metros y la necesidad de una soluci칩n eficiente, hemos elegido Hyperopt como nuestra herramienta preferida. Su enfoque basado en la optimizaci칩n bayesiana nos permite navegar este terreno con mayor precisi칩n y rapidez. En los pr칩ximos pasos de nuestra aventura, profundizaremos en el uso de Hyperopt y descubriremos c칩mo esta elecci칩n nos acerca cada vez m치s a encontrar el modelo perdido que buscamos.

<div align="right" markdown="1">
Hasta el pr칩ximo cronopunto del Principia 游볰.

DV

</div>
</div>