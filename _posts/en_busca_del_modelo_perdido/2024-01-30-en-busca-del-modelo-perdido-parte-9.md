---
title: "En Busca del Modelo Perdido - Parte 9: Explorando el Terreno de Hiperpar치metros con Grid Search e Hyperopt"
layout: single
author_profile: false
related: true
comments: true
toc: false
# date: 2024-01-30T11:48:41-04:00

sidebar:
  nav: "modelo_perdido"

classes: wide

header:
  image: /assets/images/PAES_prediction_model/busca_del_modelo_perdido_parte_9.png
  teaser: /assets/images/PAES_prediction_model/busca_del_modelo_perdido_parte_9.png

categories:
  - Machine Learning

tags:
  - XGBoost
  - Hiperpar치metros
  - Optimizaci칩n
  - Grid Search
  - Hyperopt
  - Aprendizaje Autom치tico
  - An치lisis de Datos

---

<div align="justify" markdown="1">
En nuestra inquebrantable b칰squeda del mejor modelo para predecir los resultados de la prueba PAES, nos enfrentamos a uno de los desaf칤os m치s complejos y enigm치ticos: la optimizaci칩n de hiperpar치metros en XGBoost. En este cap칤tulo abordaremos los mecanismos que tenemos a disposici칩n para buscar combinaciones de hiperpar치metros que nos ayuden a encontrar el mejor modelo de predicci칩n despu칠s de adentrarnos en este laberinto multidimensional, donde las herramientas gr치ficas son insuficientes pero nuestra imaginaci칩n y desarrollo de las matem치ticas hacen la diferencia para lograr el objetivo.

## Hiperpar치metros: El Laberinto Multidimensional

Imagina que est치s explorando un paisaje monta침oso, donde cada punto simboliza una combinaci칩n de hiperpar치metros que afectan el rendimiento de tu modelo. Comienzas en un entorno bidimensional, donde identificar el m칤nimo parece sencillo. Pero al agregar m치s hiperpar치metros, entras en un espacio tridimensional y, eventualmente, en dimensiones a칰n mayores, incrementando significativamente la complejidad de tu b칰squeda.

La visualizaci칩n en 3D que generamos (ver imagen a continuaci칩n) ilustra este primer salto a la complejidad. En ella, puedes ver c칩mo el terreno cambia con cada conjunto de hiperpar치metros. Los cortes verticales azules representan distintas perspectivas de este terreno, revelando la presencia de m칰ltiples m칤nimos locales. La l칤nea roja, nuestro gradiente, te gu칤a a trav칠s de este paisaje en busca de un m칤nimo local, partiendo de un punto espec칤fico.

<figure style="width: 100%; text-align: center;">
    <embed type="text/html" src="/assets/images/simple_post_images/minimos_superficie.html" style="width: 100%; height: 500px; border: none;" alt="Imagen 1: Heatmap del An치lisis de MAE.">
    <figcaption>Imagen 1: Ilustraci칩n del Desaf칤o de la B칰squeda de M칤nimos - Esta visualizaci칩n en 3D representa un terreno complejo que simboliza el desaf칤o de encontrar el m칤nimo absoluto en el contexto de los hiperpar치metros. Los cortes verticales azules muestran diferentes perspectivas de este terreno multidimensional, revelando m칰ltiples m칤nimos locales dependiendo del corte elegido. En este laberinto de posibilidades, explorar cada punto ser칤a una tarea costosa en t칠rminos de tiempo y recursos. Es aqu칤 donde entra en juego el "gradiente," representado por la l칤nea roja, que gu칤a la b칰squeda hacia un m칤nimo local desde un punto de partida espec칤fico. Sin embargo, el uso del gradiente puede llevarnos a m칤nimos locales en lugar del m칤nimo absoluto, lo que ilustra la complejidad de encontrar la combinaci칩n 칩ptima de hiperpar치metros para nuestro modelo.</figcaption>
</figure>

Sin embargo, esta imagen es solo una met치fora simplificada. En la realidad, trabajamos con espacios de muchas m치s dimensiones, donde la visualizaci칩n gr치fica se vuelve imposible. **En estos espacios de alta dimensi칩n, las herramientas gr치ficas pierden su utilidad y dependemos enteramente de m칠todos matem치ticos y estad칤sticos avanzados**.

Encontrar el m칤nimo absoluto en un espacio multidimensional - como el espacio que describe el MAE (Mean Absolute Error)- es un desaf칤o complejo. M치s all치 de los tres ejes que podemos visualizar, debemos aplicar algoritmos y t칠cnicas de optimizaci칩n para navegar en un terreno donde la intuici칩n visual ya no puede ayudarnos. El proceso va m치s all치 de seguir un gradiente visible; se convierte en una b칰squeda anal칤tica y matem치tica en un laberinto de posibilidades.

En esta traves칤a, nos encontramos en la encrucijada de elegir entre m칠todos que nos ayudar치n a navegar este laberinto de manera eficiente. La elecci칩n de la herramienta adecuada es fundamental, ya que nos permitir치 explorar este laberinto multidimensional con eficiencia y nos acercar치 cada vez m치s a encontrar el modelo perdido que buscamos.

## Comparando Grid Search e Hyperopt
Ahora que hemos desvelado los tesoros de nuestros hiperpar치metros y comprendido su funcionamiento ([cap칤tulo 8](https://daniavm.github.io/_posts/en_busca_del_modelo_perdido/2024-01-19-en-busca-del-modelo-perdido-parte-8/){:target="_blank"}), es hora de aprender c칩mo encontrar las combinaciones 칩ptimas para ajustar nuestro modelo XGBoost. En esta secci칩n, compararemos dos enfoques populares para optimizar los hiperpar치metros: **Grid Search e Hyperopt**.

### Grid Search: Recorriendo todo el laberinto de hiperpar치metros

Comencemos con Grid Search, un m칠todo que se asemeja a tener un mapa detallado pero limitado de nuestro laberinto de hiperpar치metros. Este enfoque es bastante simple de entender y usar, pero puede ser lento en espacios de b칰squeda extensos.

Grid Search es como explorar el espacio de hiperpar치metros siguiendo cortes precisos a trav칠s de la superficie que vimos en el gr치fico interactivo. Similar a c칩mo los cortes azules en el gr치fico representan diferentes combinaciones de hiperpar치metros, Grid Search prueba todas las combinaciones posibles dentro de un espacio de b칰squeda predefinido.

```python
from sklearn.model_selection import GridSearchCV
from xgboost import XGBRegressor

# Define el espacio de b칰squeda de hiperpar치metros
param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5]
}

# Crea un modelo base
model = XGBRegressor(random_state=42)

# Inicializa Grid Search
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, 
                           scoring='neg_mean_absolute_error', cv=5)

# Ajusta Grid Search a tus datos
grid_search.fit(X_train, y_train)

# Obtiene los mejores hiperpar치metros
best_params = grid_search.best_params_
best_mae = -grid_search.best_score_

print(f"Mejores hiperpar치metros encontrados: {best_params}")
print(f"Mejor puntuaci칩n MAE: {best_mae}")
```

### Hyperopt: Exploraci칩n "Inteligente" del Laberinto

Ahora, cambiemos nuestro enfoque hacia Hyperopt, un m칠todo que se asemeja a un explorador inteligente en nuestro laberinto. Hyperopt utiliza algoritmos basados en mecanismos bayesianos para buscar de manera m치s eficiente en el espacio de hiperpar치metros. Esto lo convierte en una excelente opci칩n cuando el espacio de b칰squeda es extenso y deseas una optimizaci칩n m치s r치pida.

Hyperopt es como seguir el gradiente en el gr치fico interactivo que presentamos. El gradiente representa la direcci칩n 칩ptima para buscar la combinaci칩n perfecta de hiperpar치metros que minimice el MAE. Sin embargo, en lugar de probar todas las combinaciones posibles, Hyperopt ajusta sus pasos utilizando informaci칩n recopilada de las iteraciones anteriores. Funciona como un explorador que aprende de sus experiencias previas y adapta su estrategia para avanzar de manera m치s efectiva hacia el m칤nimo.

En esencia, Hyperopt utiliza modelos bayesianos para estimar la funci칩n objetivo y su incertidumbre en funci칩n de las observaciones previas. Luego, elige la siguiente combinaci칩n de hiperpar치metros que maximice la informaci칩n ganada. Este enfoque permite que Hyperopt explore el espacio de b칰squeda de manera m치s inteligente y se acerque r치pidamente a las soluciones 칩ptimas.

```python
from hyperopt import hp, fmin, tpe, Trials, STATUS_OK, space_eval
from xgboost import XGBRegressor

# Define el espacio de b칰squeda de hiperpar치metros
space = {
    'n_estimators': hp.choice('n_estimators', [100, 200, 300]),
    'learning_rate': hp.loguniform('learning_rate', -3, 0),
    'max_depth': hp.quniform('max_depth', 3, 5, 1)
}

# Funci칩n objetivo para Hyperopt
def objective(params):
    model = XGBRegressor(random_state=42, n_jobs=1, **params)
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    mae = -np.mean(cross_val_score(model, X_scaled, y, cv=kf, scoring='neg_mean_absolute_error'))
    return {'loss': mae, 'status': STATUS_OK}

# Inicializa Trials para el registro
trials = Trials()

# Ejecuta Hyperopt para la optimizaci칩n
best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=N_TRIALS, trials=trials)

# Obtiene los mejores hiperpar치metros y MAE
best_params = space_eval(space, best)
best_mae = -min(trials.losses())

print(f"Mejores hiperpar치metros encontrados: {best_params}")
print(f"Mejor puntuaci칩n MAE: {best_mae}")
```

## Una Elecci칩n Importante: Definir el Espacio de B칰squeda

Es importante resaltar que la elecci칩n del espacio de b칰squeda de hiperpar치metros desempe침a un papel fundamental en el 칠xito de estas t칠cnicas ("param_grid" para el c칩digo de Grid Search y "space" para Hyperopt). Ambos m칠todos tienen la capacidad de buscar eficientemente dentro de los l칤mites que establezcamos. Por lo tanto, **definir adecuadamente estos l칤mites requiere un s칩lido entendimiento del problema y un conocimiento profundo de las caracter칤sticas estad칤sticas de los hiperpar치metros**.

La elecci칩n entre Grid Search e Hyperopt depender치 en gran medida de la complejidad del espacio de b칰squeda y de tus preferencias personales. **Grid Search es una opci칩n simple y exhaustiva, pero su velocidad puede disminuir en espacios de b칰squeda extensos**. En contraste, **Hyperopt es m치s eficiente pero demanda una configuraci칩n inicial m치s detallada para no caer en sub o sobre ajustes**. Esta elecci칩n es an치loga al dilema de recorrer minuciosamente la superficie de b칰squeda punto por punto (teniendo la certeza de vamos a encontrar la soluci칩n pero a un costo de recursos alto), o bien, decidirse a explorar de manera m치s "inteligente", utilizando herramientas locales (como la visi칩n del terreno, la gravedad, u otros) pero con la incertidumbre de saber si al punto que llegamos es verdaderamente "el punto m치s bajo".

### Elecci칩n de Hyperopt: Descubriendo el Potencial de los Datos

En nuestra b칰squeda para construir el mejor modelo que prediga los resultados de la prueba PAES, hemos hecho un hallazgo revelador. M치s all치 de los ensayos de los estudiantes, contamos con una amplia gama de informaci칩n adicional, como anotaciones conductuales, asistencia, registros de psicopedagog칤a y m치s. Los cuales podemos agregar a este modelo.

Este descubrimiento nos lleva a la elecci칩n de Hyperopt como nuestra herramienta de optimizaci칩n de hiperpar치metros. Hyperopt es capaz de manejar eficientemente espacios de hiperpar치metros extensos y aprender en tiempo real, lo que es fundamental en un entorno de datos tan rico como el escolar.

Nuestra elecci칩n de Hyperopt se basa en su capacidad para abordar la complejidad de nuestros datos y su eficiencia en el uso de recursos. Esto nos permite incorporar m치s datos y mejorar nuestro modelo para que XGBoost, nuestro 'm칰sico obsesivo por la perfecci칩n', pueda aprovechar al m치ximo la informaci칩n disponible y perfeccionar su t칠cnica.

En el pr칩ximo cap칤tulo, definiremos los l칤mites del espacio de b칰squeda en Hyperopt, un paso crucial en nuestro camino hacia el modelo perdido.

<div align="right" markdown="1">
Hasta el pr칩ximo cronopunto del Principia 游볰.

DV

</div>
</div>