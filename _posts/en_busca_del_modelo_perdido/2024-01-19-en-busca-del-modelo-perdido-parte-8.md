---
title: "En Busca del Modelo Perdido - Parte 8: El Laberinto de XGBoost"
layout: single
author_profile: false
related: true
comments: true
toc: false
date: 2024-01-19T11:48:41-04:00

sidebar:
  nav: "modelo_perdido"

classes: wide

header:
  image: /assets/images/PAES_prediction_model/busca_del_modelo_perdido_parte_8.png
  teaser: /assets/images/PAES_prediction_model/busca_del_modelo_perdido_parte_8.png

categories:
  - Machine Learning

tags:
  - XGBoost
  - Hiperpar치metros
  - Sobreajuste
  - Validaci칩n Cruzada
  - Predicci칩n de PAES
  - Aprendizaje Autom치tico
  - An치lisis de Datos
  - T칠cnicas de Boosting

---
<div align="justify" markdown="1">
Mientras continuamos nuestra odisea en "En Busca del Modelo Perdido", nos adentramos en un territorio inexplorado en el cap칤tulo ocho: el mundo de XGBoost. Antes de sumergirnos en el laberinto de sus hiperpar치metros, es esencial comprender qu칠 es XGBoost y c칩mo se diferencia de nuestro anterior compa침ero de viaje, Random Forest.

## 游꺕 XGBoost: Un Nuevo Horizonte en Modelos Basados en 츼rboles 游꺕
XGBoost, que significa eXtreme Gradient Boosting, es una evoluci칩n sofisticada de los modelos basados en 치rboles de decisi칩n. Mientras que Random Forest construye un 'bosque' de 치rboles de decisi칩n independientes para crear predicciones robustas y generalizadas, XGBoost perfecciona esta idea utilizando el enfoque de Boosting. Aqu칤, cada 치rbol se construye secuencialmente, aprendiendo y mejorando de los errores del anterior, lo que lo convierte en un solista meticuloso en nuestra orquesta de modelos de Machine Learning.

Ahora, con este entendimiento de XGBoost, nos adentramos en su aspecto m치s desafiante y crucial: los hiperpar치metros.

## 游 XGBoost: Un Nuevo Cap칤tulo en Nuestra Saga 游
Recordando nuestro viaje anterior, vimos c칩mo Random Forest, representando el Bagging, funcionaba como una orquesta sinf칩nica, donde cada instrumento contribu칤a a una melod칤a robusta. Ahora, con XGBoost, nos movemos hacia un enfoque m치s solista y perfeccionista del Boosting, destacado en el cap칤tulo 7.

```python
import xgboost as xgb
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_absolute_error

# Supongamos que 'X' y 'y' son nuestros datos de caracter칤sticas y etiquetas
X = ... # Caracter칤sticas del conjunto de datos PAES
y = ... # Puntajes PAES

# Configuramos el modelo XGBoost con hiperpar치metros seleccionados
xg_model = xgb.XGBRegressor(
    learning_rate=0.1,
    n_estimators=350,
    gamma=2.5,
    min_child_weight=7,
    subsample=0.5,
    colsample_bytree=0.5,
    colsample_bylevel=0.5,
    colsample_bynode=0.5,
    reg_lambda=5,
    max_depth=5,
    random_state=42
)

# Validaci칩n cruzada para evaluar el modelo
scores = cross_val_score(xg_model, X, y, cv=5, scoring='neg_mean_absolute_error')
mae = -scores.mean()
print("MAE promedio: %f" % (mae))
```

En este fragmento, aplicamos validaci칩n cruzada para evaluar nuestro modelo. A primera vista, podr칤amos obtener un gr치fico que muestra resultados impresionantes, sugiriendo una predicci칩n casi perfecta. Sin embargo, debemos ser cautelosos: 쯘s esto realmente una muestra de precisi칩n o hemos ca칤do en la trampa del sobreajuste? 

Para hacerlo notar, aqu칤 he aplicado este modelo con algunos de los par치metros dados:

<figure style = "float: center; width: 100%; text-align: center; font-style: italic; font-size: 0.8em; text-indent: 0; margin: 0.6em; padding: 0.8em;">
  <a href="/assets/images/PAES_prediction_model/ejemplo_xgboost_cap8.png">
    <img src="/assets/images/PAES_prediction_model/ejemplo_xgboost_cap8.png" width="100%"  alt="Imagen 1: Resultado de ajuste por XGBoost para los datos de la predicci칩n buscada en la prueba de matem치tica 1. El eje izquierdo muestra la predicci칩n y el inferior los valores reales de las pruebas consideradas. EL error aproximado entre la predicci칩n y los valores reales fue de unos 15 puntos, lo que representa 1 respuesta correcta de error en la predicci칩n.">
  </a>
  <figcaption>Imagen 1: Resultado de ajuste por XGBoost para los datos de la predicci칩n buscada en la prueba de matem치tica 1. El eje izquierdo muestra la predicci칩n y el inferior los valores reales de las pruebas consideradas. EL error aproximado entre la predicci칩n y los valores reales fue de unos 15 puntos, lo que representa 1 respuesta correcta de error en la predicci칩n.</figcaption>
</figure>

Este gr치fico puede parecer prometedor (de hecho llega a ser casi perfecto en su linealidad), pero en realidad, puede ser una ilusi칩n del verdadero rendimiento del modelo, especialmente en datos desconocidos. El modelo se ajusta muy bien a los datos qeu tenemos y a la informaci칩n que ya es sabida, pero lo m치s probable es que no se ajuste de ninguna forma a los datos que nos son desconocidos, es decir, no se ajustar치n bien para hacer una predicci칩n. 

## 游댌 Navegando por los Hiperpar치metros de XGBoost 游댌
Cada hiperpar치metro de XGBoost es un artefacto crucial en nuestra aventura, similar a los que Indiana Jones encontrar칤a en sus expediciones. Los exploramos detalladamente, comprendiendo su funci칩n y c칩mo afectan nuestras predicciones:

- **Learning Rate (Tasa de Aprendizaje)**: Controla el paso con el que el modelo se ajusta a los datos. Un paso demasiado grande puede hacer que el modelo se pierda detalles finos.
- **N_Estimators (N칰mero de Estimadores)**: El n칰mero de 치rboles de decisi칩n en el modelo. M치s 치rboles pueden llevar a un mayor riesgo de sobreajuste.
- **Gamma**: Un par치metro de regularizaci칩n que ayuda a controlar la complejidad del modelo.
- **Min Child Weight**: Un umbral para decidir cu치ndo se debe detener la divisi칩n de un nodo, afectando la complejidad del modelo.
- **Subsample**: La fracci칩n de muestras utilizadas para entrenar cada 치rbol. Un subsample bajo puede llevar a un modelo m치s robusto.
- **Colsample_by_**: Estos par치metros controlan la fracci칩n de caracter칤sticas consideradas para cada 치rbol, nivel o divisi칩n. Una selecci칩n m치s peque침a puede prevenir el sobreajuste.

## 游 El Desaf칤o de XGBoost: Evitando las Trampas del Sobreajuste 游
A diferencia de Random Forest, XGBoost puede ser susceptible al sobreajuste. Es como si Indiana Jones enfrentara una serie de espejismos mientras busca un tesoro. Necesitamos ajustar los hiperpar치metros cuidadosamente para evitar seguir un mapa enga침oso.

## 游늳 Pr칩ximo Cap칤tulo: Ajustando el Comp치s hacia la Predicci칩n Perfecta 游늳
En el pr칩ximo cap칤tulo, nos enfocaremos en ajustar estos hiperpar치metros para encontrar un equilibrio entre la precisi칩n y la capacidad de generalizaci칩n, evitando las trampas del sobreajuste. Cada paso ser치 crucial en nuestra b칰squeda del Santo Grial de la predicci칩n de los resultados de la PAES.

<div align="right" markdown="1">
Hasta el pr칩ximo cronopunto del Principia 游볰.

DV

</div>
</div>